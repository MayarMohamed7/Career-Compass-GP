{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJda4Ds_rsNt",
        "outputId": "3ec5d894-69e0-4e72-e859-497039f20f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYzVDMZQFJss",
        "outputId": "2ec8e574-4be0-4824-ff1f-1810da68409b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azr5w1qAEkVK",
        "outputId": "61894a8d-8116-423e-89c0-28adc650ba20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpAorqkDmc53"
      },
      "source": [
        "**training word2vec model on our dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrutALg4mh0s",
        "outputId": "e8caf5c3-5501-4293-db36-d3a8b51c0d78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-8a2564367672>:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['Skills'] = df['Skills'].str.replace('|', ',').str.lower()\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "\n",
        "df =  pd.read_csv(\"/content/drive/MyDrive/CareerCoach_Dataset.csv\")\n",
        "df['Skills'] = df['Skills'].str.replace('|', ',').str.lower()\n",
        "tokenized_skills = [skill_set.split() for skill_set in df['Skills']]\n",
        "\n",
        "# Train the Word2Vec model on the tokenized data\n",
        "word2vec_model = Word2Vec(sentences=tokenized_skills, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word2vec_model.save(\"word2vec_skills.model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coQUeqkVi89W"
      },
      "source": [
        "Skill extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AawqpNsHi5Vj",
        "outputId": "3d1f617a-946b-4db2-8c72-83d3c9f8b814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probable Skills: ['c++', 'java', 'node_js', 'machine_learning', 'python']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "#spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "skill_df = pd.read_csv(\"/content/drive/MyDrive/dataset_grad/first_trans_try.csv\")\n",
        "skill_keywords = skill_df[\"Skills\"].astype(str).str.lower().tolist()\n",
        "ignore_words = set([\"me\", \"and\", \"or\", \"i\", \"myself\", \"experience\", \"excellent\", \"skill\", \"strong\", \"good\", \"be\", \"using\",\"use\", \"skills\"])\n",
        "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "patterns = [nlp.make_doc(text) for text in skill_keywords if text not in ignore_words]\n",
        "matcher.add(\"SKILL_PATTERNS\", patterns)\n",
        "\n",
        "\n",
        "def extract_entities_skills_and_bigrams(text):\n",
        "    doc = nlp(text)\n",
        "    probable_skills = set()\n",
        "\n",
        "   #phrasematcher 3shan l multi word skill\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        span_text = span.text.lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
        "        probable_skills.add(span_text)\n",
        "\n",
        "    # NER and ngrams\n",
        "    for ent in doc.ents:\n",
        "      ent_text = ent.text.lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
        "      if ent_text not in ignore_words and ent_text in skill_keywords:\n",
        "        probable_skills.add(ent_text)\n",
        "    for token1, token2 in zip(doc[:-1], doc[1:]):\n",
        "        bigram_text = f\"{token1.text.lower()} {token2.text.lower()}\"\n",
        "        bigram_key = bigram_text.replace(\" \", \"_\").replace(\".\", \"_\")\n",
        "        if bigram_text in skill_keywords and bigram_key not in ignore_words:\n",
        "            probable_skills.add(bigram_key)\n",
        "\n",
        "    # Use PoS tagging to identify nouns and verbs as probable skills\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB']:\n",
        "            token_text = token.lemma_.lower().replace(\" \", \"_\").replace(\".\", \"_\")\n",
        "            if token_text not in ignore_words and token_text in skill_keywords and token_text not in probable_skills:\n",
        "                probable_skills.append(token_text)\n",
        "\n",
        "    #n filter l hagat ele tl3t ba\n",
        "    final_skills = set()\n",
        "    for skill in probable_skills:\n",
        "      if any(skill in multi_word_skill for multi_word_skill in probable_skills if multi_word_skill != skill):\n",
        "        continue #y3ny lw l'a l skill de mwgoda f mukti word yskipha w myhothash tany\n",
        "      final_skills.add(skill)\n",
        "    return list(final_skills)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "user_paragraph = \"My skills are java, python, C++, machine learning and node js\"\n",
        "probable_skills = extract_entities_skills_and_bigrams(user_paragraph)\n",
        "\n",
        "print (\"Probable Skills:\", probable_skills)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtR2MHKKtyq-"
      },
      "source": [
        "**use the trained W2V model to transform user skills into vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzqzBQbWtv3A",
        "outputId": "9a1ce7e7-8401-492c-f888-764801d8c5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[ 1.28334478e-01 -3.19948971e-01 -3.87016714e-01 -1.31472483e-01\n",
            "    2.41616485e-03 -1.40886828e-01 -3.41250658e-01  1.33707196e-01\n",
            "    6.74414992e-01  2.31944229e-02 -4.42652911e-01  1.04114890e+00\n",
            "   -1.33193269e-01  5.80707490e-02  4.38426696e-02  4.84390736e-01\n",
            "   -2.92658389e-01 -1.45168513e-01 -7.19160616e-01 -6.88091159e-01\n",
            "   -3.21424782e-01 -5.02774179e-01  2.05604807e-01  5.28691232e-01\n",
            "    9.73938227e-01  1.86664119e-01 -5.38564175e-02  4.57600772e-01\n",
            "   -2.98110787e-02 -1.74651951e-01 -8.71968865e-01 -1.27052054e-01\n",
            "    4.35195088e-01 -6.21793196e-02  7.98062146e-01  3.75206321e-01\n",
            "   -3.47611815e-01 -1.14078082e-01  1.90170407e-02 -1.41919658e-01\n",
            "    2.64568999e-02 -1.45481462e-02  5.55108011e-01  6.93215728e-02\n",
            "   -4.56465304e-01 -1.71041191e-01  3.77587080e-01  1.95442997e-02\n",
            "    1.59004748e-01  1.18379569e+00 -1.93044707e-01 -2.60847509e-01\n",
            "   -9.11563858e-02 -6.99881494e-01  6.27568901e-01  2.61450976e-01\n",
            "    1.28587902e-01  3.74561667e-01 -1.25633955e-01  5.20230114e-01\n",
            "    1.06047213e-01 -1.04120843e-01 -6.09308362e-01 -3.62519681e-01\n",
            "    2.01966614e-01  4.72479872e-02  5.38620651e-01  6.07981384e-01\n",
            "   -2.74639249e-01 -1.63856164e-01 -9.57853394e-04  1.10437244e-01\n",
            "    3.20411384e-01  4.76007462e-01 -6.16877496e-01 -8.10143471e-01\n",
            "    1.80014491e-01  2.76614189e-01  3.62660199e-01 -3.04352552e-01\n",
            "    1.23756312e-01  7.70282090e-01 -1.50441915e-01  2.56434351e-01\n",
            "   -1.64105579e-01  3.39442790e-01  3.06167245e-01  6.70477971e-02\n",
            "   -3.50225806e-01 -2.97948569e-02 -2.56930530e-01  2.47192606e-01\n",
            "    4.66975057e-03  3.03319871e-01  4.42388617e-02 -1.82636395e-01\n",
            "   -9.26059932e-02  1.26355276e-01  3.48873615e-01 -7.16797039e-02]]\n",
            "\n",
            " [[ 3.27482653e+00  1.23221350e+00 -1.57601386e-01  3.59754562e+00\n",
            "    2.59235978e+00  3.74553919e+00 -1.81598231e-01  9.64604318e-01\n",
            "   -2.54208422e+00 -4.16526031e+00 -4.04296398e+00  1.02454960e+00\n",
            "   -4.88560581e+00  1.12024009e+00 -2.05202174e+00  4.67725337e-01\n",
            "    3.09421754e+00  1.36229545e-01 -2.10393742e-01  6.94281244e+00\n",
            "    1.95310855e+00  1.96238244e+00 -4.29296553e-01 -1.50079060e+00\n",
            "    8.14505577e-01  6.44596338e+00  1.51803267e+00  6.66072726e-01\n",
            "    2.70608783e+00 -1.98892336e-02 -1.34852076e+00 -8.54896247e-01\n",
            "   -7.52023602e+00  4.01477957e+00 -3.82857394e+00  2.54212737e+00\n",
            "   -9.59798336e-01  5.07635927e+00 -1.92618108e+00 -1.80174029e+00\n",
            "   -3.80586934e+00  1.92207301e+00  8.80315840e-01  2.47924352e+00\n",
            "    7.84459651e-01 -7.63943529e+00  2.09329677e+00 -3.22277069e-01\n",
            "   -3.10960627e+00  6.16139114e-01  4.35796452e+00  3.19526434e+00\n",
            "    1.03093016e+00  7.40852642e+00 -9.58335400e-01 -4.69790792e+00\n",
            "   -2.00942349e+00  4.76606321e+00  9.89615726e+00 -1.19558787e+00\n",
            "   -3.49327636e+00 -4.35425997e+00  1.75951219e+00 -7.69036472e-01\n",
            "   -2.61065888e+00  4.37361896e-01  5.19862473e-01 -2.79424691e+00\n",
            "   -7.94604301e+00 -5.33557844e+00  3.04418623e-01 -6.68592215e-01\n",
            "   -6.36232519e+00  3.00025735e-02  2.04979229e+00  4.38822412e+00\n",
            "    7.13868618e+00 -3.91582346e+00 -1.47850561e+00  2.23341560e+00\n",
            "    3.58973622e-01 -2.82833719e+00 -2.41554332e+00 -4.04727364e+00\n",
            "    1.15286219e+00 -1.46149909e+00  1.92802751e+00  1.05938864e+00\n",
            "    3.30559731e+00  2.74124765e+00 -3.64919519e+00  2.31221080e+00\n",
            "    2.56668639e+00  8.33706915e-01 -2.47342563e+00  3.24141073e+00\n",
            "   -2.41673127e-01  3.04078341e+00  2.03044677e+00 -9.40444291e-01]]\n",
            "\n",
            " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "\n",
            " [[-2.29070807e+00  5.15100539e-01 -2.43741894e+00  4.18377638e+00\n",
            "    1.62017077e-01  2.29394659e-01  3.54798460e+00 -1.61822975e+00\n",
            "    1.27738762e+00  1.41285503e+00  2.99491704e-01 -2.77138019e+00\n",
            "    2.58938861e+00 -4.09301132e-01  1.79000273e-01 -1.60786724e+00\n",
            "    5.86049914e-01 -7.77094185e-01 -4.28879499e-01 -1.92706895e+00\n",
            "   -4.27113104e+00  2.54196338e-02  3.23539996e+00  5.80614567e-01\n",
            "   -5.17636061e-01 -1.72653592e+00  7.35968053e-02  1.44006193e+00\n",
            "   -1.66609955e+00  1.40759408e-01 -1.65738249e+00 -2.60058586e-02\n",
            "   -7.16991246e-01 -1.49807072e+00 -2.00320527e-01 -1.92324519e-01\n",
            "   -2.05019999e+00 -1.30597234e+00 -3.19320232e-01  1.78355551e+00\n",
            "   -1.49361357e-01 -5.55931509e-01  2.99480414e+00 -1.34167874e+00\n",
            "    1.03979242e+00 -1.88243642e-01  1.92510426e+00  1.49961203e-01\n",
            "   -1.10900509e+00  1.39214098e+00  2.65676236e+00 -1.51962757e+00\n",
            "    5.85883200e-01 -1.97036362e+00  1.61219561e+00 -1.54016244e+00\n",
            "   -1.70306301e+00 -1.54439104e+00  4.04882240e+00  3.18749189e+00\n",
            "    1.48076129e+00  9.05070364e-01  1.62378979e+00  3.53682876e+00\n",
            "   -3.03948641e+00  9.73976433e-01 -2.91459799e-01  1.73725986e+00\n",
            "    3.89499009e-01 -1.63992479e-01 -3.12168908e+00  3.72703457e+00\n",
            "    5.92898071e-01 -3.05208945e+00  4.29239184e-01  1.30211079e+00\n",
            "   -2.46299839e+00  3.20313603e-01  1.07268202e+00 -3.86043572e+00\n",
            "    1.13508868e+00  2.36907935e+00 -9.86590981e-01 -1.22876084e+00\n",
            "   -1.61643362e+00 -2.43161535e+00 -1.15605164e+00 -6.58277571e-01\n",
            "    3.55217981e+00 -2.07189965e+00  4.58185434e-01 -3.04443073e+00\n",
            "    6.41246498e-01  1.27684808e+00 -7.85410553e-02  2.21806908e+00\n",
            "   -2.57922679e-01 -3.52447212e-01 -9.73195553e-01 -2.47212052e+00]]]\n",
            "skills [array([[ 1.28334478e-01, -3.19948971e-01, -3.87016714e-01,\n",
            "        -1.31472483e-01,  2.41616485e-03, -1.40886828e-01,\n",
            "        -3.41250658e-01,  1.33707196e-01,  6.74414992e-01,\n",
            "         2.31944229e-02, -4.42652911e-01,  1.04114890e+00,\n",
            "        -1.33193269e-01,  5.80707490e-02,  4.38426696e-02,\n",
            "         4.84390736e-01, -2.92658389e-01, -1.45168513e-01,\n",
            "        -7.19160616e-01, -6.88091159e-01, -3.21424782e-01,\n",
            "        -5.02774179e-01,  2.05604807e-01,  5.28691232e-01,\n",
            "         9.73938227e-01,  1.86664119e-01, -5.38564175e-02,\n",
            "         4.57600772e-01, -2.98110787e-02, -1.74651951e-01,\n",
            "        -8.71968865e-01, -1.27052054e-01,  4.35195088e-01,\n",
            "        -6.21793196e-02,  7.98062146e-01,  3.75206321e-01,\n",
            "        -3.47611815e-01, -1.14078082e-01,  1.90170407e-02,\n",
            "        -1.41919658e-01,  2.64568999e-02, -1.45481462e-02,\n",
            "         5.55108011e-01,  6.93215728e-02, -4.56465304e-01,\n",
            "        -1.71041191e-01,  3.77587080e-01,  1.95442997e-02,\n",
            "         1.59004748e-01,  1.18379569e+00, -1.93044707e-01,\n",
            "        -2.60847509e-01, -9.11563858e-02, -6.99881494e-01,\n",
            "         6.27568901e-01,  2.61450976e-01,  1.28587902e-01,\n",
            "         3.74561667e-01, -1.25633955e-01,  5.20230114e-01,\n",
            "         1.06047213e-01, -1.04120843e-01, -6.09308362e-01,\n",
            "        -3.62519681e-01,  2.01966614e-01,  4.72479872e-02,\n",
            "         5.38620651e-01,  6.07981384e-01, -2.74639249e-01,\n",
            "        -1.63856164e-01, -9.57853394e-04,  1.10437244e-01,\n",
            "         3.20411384e-01,  4.76007462e-01, -6.16877496e-01,\n",
            "        -8.10143471e-01,  1.80014491e-01,  2.76614189e-01,\n",
            "         3.62660199e-01, -3.04352552e-01,  1.23756312e-01,\n",
            "         7.70282090e-01, -1.50441915e-01,  2.56434351e-01,\n",
            "        -1.64105579e-01,  3.39442790e-01,  3.06167245e-01,\n",
            "         6.70477971e-02, -3.50225806e-01, -2.97948569e-02,\n",
            "        -2.56930530e-01,  2.47192606e-01,  4.66975057e-03,\n",
            "         3.03319871e-01,  4.42388617e-02, -1.82636395e-01,\n",
            "        -9.26059932e-02,  1.26355276e-01,  3.48873615e-01,\n",
            "        -7.16797039e-02]], dtype=float32), array([[ 3.2748265 ,  1.2322135 , -0.15760139,  3.5975456 ,  2.5923598 ,\n",
            "         3.7455392 , -0.18159823,  0.9646043 , -2.5420842 , -4.1652603 ,\n",
            "        -4.042964  ,  1.0245496 , -4.885606  ,  1.1202401 , -2.0520217 ,\n",
            "         0.46772534,  3.0942175 ,  0.13622954, -0.21039374,  6.9428124 ,\n",
            "         1.9531085 ,  1.9623824 , -0.42929655, -1.5007906 ,  0.8145056 ,\n",
            "         6.4459634 ,  1.5180327 ,  0.6660727 ,  2.7060878 , -0.01988923,\n",
            "        -1.3485208 , -0.85489625, -7.520236  ,  4.0147796 , -3.828574  ,\n",
            "         2.5421274 , -0.95979834,  5.0763593 , -1.9261811 , -1.8017403 ,\n",
            "        -3.8058693 ,  1.922073  ,  0.88031584,  2.4792435 ,  0.78445965,\n",
            "        -7.6394353 ,  2.0932968 , -0.32227707, -3.1096063 ,  0.6161391 ,\n",
            "         4.3579645 ,  3.1952643 ,  1.0309302 ,  7.4085264 , -0.9583354 ,\n",
            "        -4.697908  , -2.0094235 ,  4.766063  ,  9.896157  , -1.1955879 ,\n",
            "        -3.4932764 , -4.35426   ,  1.7595122 , -0.7690365 , -2.610659  ,\n",
            "         0.4373619 ,  0.5198625 , -2.794247  , -7.946043  , -5.3355784 ,\n",
            "         0.30441862, -0.6685922 , -6.362325  ,  0.03000257,  2.0497923 ,\n",
            "         4.388224  ,  7.138686  , -3.9158235 , -1.4785056 ,  2.2334156 ,\n",
            "         0.35897362, -2.8283372 , -2.4155433 , -4.0472736 ,  1.1528622 ,\n",
            "        -1.4614991 ,  1.9280275 ,  1.0593886 ,  3.3055973 ,  2.7412477 ,\n",
            "        -3.6491952 ,  2.3122108 ,  2.5666864 ,  0.8337069 , -2.4734256 ,\n",
            "         3.2414107 , -0.24167313,  3.0407834 ,  2.0304468 , -0.9404443 ]],\n",
            "      dtype=float32), array([], dtype=float64), array([], dtype=float64), array([[-2.290708  ,  0.51510054, -2.437419  ,  4.1837764 ,  0.16201708,\n",
            "         0.22939466,  3.5479846 , -1.6182297 ,  1.2773876 ,  1.412855  ,\n",
            "         0.2994917 , -2.7713802 ,  2.5893886 , -0.40930113,  0.17900027,\n",
            "        -1.6078672 ,  0.5860499 , -0.7770942 , -0.4288795 , -1.927069  ,\n",
            "        -4.271131  ,  0.02541963,  3.2354    ,  0.58061457, -0.51763606,\n",
            "        -1.7265359 ,  0.07359681,  1.4400619 , -1.6660995 ,  0.14075941,\n",
            "        -1.6573825 , -0.02600586, -0.71699125, -1.4980707 , -0.20032053,\n",
            "        -0.19232452, -2.0502    , -1.3059723 , -0.31932023,  1.7835555 ,\n",
            "        -0.14936136, -0.5559315 ,  2.9948041 , -1.3416787 ,  1.0397924 ,\n",
            "        -0.18824364,  1.9251043 ,  0.1499612 , -1.1090051 ,  1.392141  ,\n",
            "         2.6567624 , -1.5196276 ,  0.5858832 , -1.9703636 ,  1.6121956 ,\n",
            "        -1.5401624 , -1.703063  , -1.544391  ,  4.0488224 ,  3.187492  ,\n",
            "         1.4807613 ,  0.90507036,  1.6237898 ,  3.5368288 , -3.0394864 ,\n",
            "         0.97397643, -0.2914598 ,  1.7372599 ,  0.389499  , -0.16399248,\n",
            "        -3.121689  ,  3.7270346 ,  0.5928981 , -3.0520895 ,  0.42923918,\n",
            "         1.3021108 , -2.4629984 ,  0.3203136 ,  1.072682  , -3.8604357 ,\n",
            "         1.1350887 ,  2.3690794 , -0.986591  , -1.2287608 , -1.6164336 ,\n",
            "        -2.4316154 , -1.1560516 , -0.6582776 ,  3.5521798 , -2.0718997 ,\n",
            "         0.45818543, -3.0444307 ,  0.6412465 ,  1.2768481 , -0.07854106,\n",
            "         2.218069  , -0.25792268, -0.3524472 , -0.97319555, -2.4721205 ]],\n",
            "      dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec.load(\"word2vec_skills.model\")\n",
        "\n",
        "\n",
        "def skill_to_vector(skill, model):\n",
        "    skill = skill.replace('_', ' ')  # Convert back to space-separated\n",
        "    if skill in model.wv:\n",
        "        return model.wv[skill]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def skills_to_vector_sequence(skill_list, model):\n",
        "    vectors = [skill_to_vector(skill, model) for skill in skill_list]\n",
        "    vectors = [vec for vec in vectors if vec is not None]  # Remove skills not in the Word2Vec vocabulary\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Example usage with probable_skills extracted from user input\n",
        "probable_skill_vectors = [skills_to_vector_sequence([skill], word2vec_model) for skill in probable_skills]\n",
        "\n",
        "# Determine the maximum sequence length\n",
        "max_seq_length = max(len(skill_seq) for skill_seq in probable_skill_vectors)\n",
        "\n",
        "# Pad sequences\n",
        "padded_skill_sequences = pad_sequences(probable_skill_vectors, maxlen=max_seq_length, padding='post', dtype='float32')\n",
        "\n",
        "print (padded_skill_sequences)\n",
        "print (\"skills\",probable_skill_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XCC2VyW6NIMn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming df['Skills'] contains the skill sets for each entry in the dataset\n",
        "# Transform each skill set into a sequence of vectors\n",
        "dataset_skill_sequences = []\n",
        "for skills_str in df['Skills']:\n",
        "    # Split the skills string into individual skills\n",
        "    skills_list = skills_str.split('|')  # Adjust split method based on your data formatting\n",
        "    # Convert skills to vectors\n",
        "    skill_vectors = skills_to_vector_sequence(skills_list, word2vec_model)\n",
        "    dataset_skill_sequences.append(skill_vectors)\n",
        "\n",
        "# Determine the maximum sequence length across all entries in the dataset\n",
        "max_length_dataset = max(len(skill_seq) for skill_seq in dataset_skill_sequences)\n",
        "\n",
        "# Pad the sequences for the entire dataset\n",
        "padded_dataset_sequences = pad_sequences(dataset_skill_sequences, maxlen=max_length_dataset, padding='post', dtype='float32')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw0B59cX5_wo"
      },
      "source": [
        "**Encoding Job titles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQUGTG-45-Mx",
        "outputId": "6c07f5d5-4f6f-432f-cdde-f56bb01e953b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "job_titles_encoded = encoder.fit_transform(df[['Job Title']])\n",
        "job_titles_df = pd.DataFrame(job_titles_encoded, columns=encoder.get_feature_names_out(['Job Title']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_GmLaLxEojQ",
        "outputId": "a1dc6114-998e-47aa-d051-71e651b4601b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of padded skill sequences: 374134\n",
            "Number of job titles encoded: 374134\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of padded skill sequences:\", len(padded_dataset_sequences))\n",
        "print(\"Number of job titles encoded:\", len(job_titles_encoded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pJvEmGSQDPm6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "assert len(padded_dataset_sequences) == len(job_titles_encoded), \"The number of skill sequences and job titles must match.\" #error msg\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_dataset_sequences, job_titles_encoded, test_size=0.2, random_state=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r99_QN66dB_"
      },
      "source": [
        "**Model training LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLZJ3v4P6cy5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(units=y_train.shape[1], activation='softmax'))  # Output layer\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
